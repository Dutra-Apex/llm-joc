{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dutra-Apex/llm-joc/blob/main/mistral-model/Mistral_7b_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to download Mistral LLM and link it to google colab\n",
        "\n",
        "Source: https://github.com/shobhitag11/Mistral_LLM_Demo/tree/main\n",
        "\n",
        "1.   Download the model into your local system. Please note you need have git and git-lfs installed on your system.\n",
        "\n",
        "*   git lfs install\n",
        "*   git clone https://huggingface.co/mistralai/Mistral-7B-v0.1\n",
        "\n",
        "2.  Now, remove the .git folder from the downloaded folder path, which might be consuming a lot of GB.\n",
        "\n",
        "3.  Upload the downloaded LLM model folder to your google drive- if the space is less on your GDrive, you may opt for google membership free of cost(google provides 3 months of trail pack which comes with 100 GBs of GDrive space)\n",
        "\n",
        "4.  Make sure to change runtime type to GPU\n",
        "\n",
        "5.  Link Google Drive to your colab notebook as follows\n",
        "\n"
      ],
      "metadata": {
        "id": "JHIg4pdc6FDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount GDrive to Your Colab Notebook"
      ],
      "metadata": {
        "id": "HeCki7jm8Djt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tkneP4lvQkZ",
        "outputId": "a7718c7a-efa6-4b43-febd-90283013b5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Alternative: Directly download Mistral Model from Huggingface website\n",
        "#### Here the challenge is, if you want to experiment this on free google colab notebook, the notebook might crash(`which happened to me as well, so that's why I created a trick mentioned in above step`) since the model is huge and even to download the model you might require lot of RAM and Space."
      ],
      "metadata": {
        "id": "E-sJohWo58yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Download model directly to Colab Workspace"
      ],
      "metadata": {
        "id": "lYeD0G3787Ki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyi644Wa8uG7",
        "outputId": "85d82688-4060-4c0e-c3bc-ddee969a1d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Mistral-7B-v0.1'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 87 (delta 43), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (87/87), 473.17 KiB | 1.76 MiB/s, done.\n",
            "error: unable to write file model-00001-of-00002.safetensors\n",
            "Filtering content: 100% (5/5), 2.20 GiB | 1.69 MiB/s, done.\n",
            "fatal: unable to checkout working tree\n",
            "Encountered 4 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00002-of-00002.safetensors\n",
            "\tpytorch_model-00002-of-00002.bin\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\tmodel-00001-of-00002.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/mistralai/Mistral-7B-v0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Don't forget to remove .git folder to get rid of unwanted space consumed"
      ],
      "metadata": {
        "id": "-vfEx_2c9Cia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf /content/Mistral-7B-v0.1/.git"
      ],
      "metadata": {
        "id": "gnCSpBwa_K3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Packages"
      ],
      "metadata": {
        "id": "eKB2Enwa9M5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch datasets\n",
        "!pip install -q accelerate==0.21.0 \\\n",
        "                peft==0.4.0 \\\n",
        "                bitsandbytes==0.40.2 \\\n",
        "                transformers==4.31.0 \\\n",
        "                trl==0.4.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKAbx75wCA-5",
        "outputId": "012e5e8c-7af8-4ec3-d51d-6ee190d90838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement trl==0.4.7? (from versions: 0.0.1, 0.0.2, 0.0.3, 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.3.1, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.5.0, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for trl==0.4.7?\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9iRqc69EUo1",
        "outputId": "19403019-091d-4e9e-a849-a6650e79a37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.25.0 bitsandbytes-0.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if the GPU is enabled?"
      ],
      "metadata": {
        "id": "Dn8jmDdn9X0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7eGAO3rFfyo",
        "outputId": "ba52c166-f043-4192-bc7e-55a5da1b6443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  8 02:47:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "lm-OGay_9b3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")"
      ],
      "metadata": {
        "id": "P5QzimXLv4qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the device to cuda, to Load model on GPUs `since we have a single GPU it is by default set to cuda:0`"
      ],
      "metadata": {
        "id": "VaiGaiox9d_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device('cuda')"
      ],
      "metadata": {
        "id": "Hn3thN7qzrlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tokenizer and model from the linked google drive path"
      ],
      "metadata": {
        "id": "YYWvl4799vMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model with 8 bit quantization, since it might crash with float16 precision.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Mistral-7B-v0.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Mistral-7B-v0.1/\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,# For 8 bit quantization,\n",
        "    max_memory={0:\"15GB\"}\n",
        ")\n",
        "model.eval()\n",
        "model = torch.compile(model, mode = \"max-autotune\", backend=\"inductor\")"
      ],
      "metadata": {
        "id": "eMqXrhavDvwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DymSaNK_3207",
        "outputId": "f9400c33-6d89-414c-9b69-753b2d22dce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Model: Question 1\n"
      ],
      "metadata": {
        "id": "p6RAzFtU-Bei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is the capital of India?\"\n",
        "\n",
        "device = 'cuda'\n",
        "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, temperature=0.1, top_k=1, top_p=1.0, repetition_penalty=1.4, min_new_tokens=16, max_new_tokens=128, do_sample=True)\n",
        "decoded = tokenizer.decode(generated_ids[0])\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "BmN0DK3fEP7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978c49c1-8dc1-4fd7-91e9-5543c0710ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> What is the capital of India?\n",
            "New Delhi. It was established as a temporary national capital in 1930 and became permanent after independence from Britain on August 2, 1846. The city has been home to many important events including Gandhi’s assassination (January 7th), Jawaharlal Nehru University riots(June-August)and more recently protests against corruption by Anna Hazare . In addition there are also several museums such as National Museum which houses artifacts dating back thousands years ago! There were even some rumors about an underground tunnel system connecting all these places together but no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the final LLM Generated Answer"
      ],
      "metadata": {
        "id": "zfY_-6IF-E86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = decoded.split(text)[-1]\n",
        "answer = answer.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqdrJi0e1JeP",
        "outputId": "2f4e7087-d616-4b1d-9c49-dcfc5968c092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New Delhi. It was established as a temporary national capital in 1930 and became permanent after independence from Britain on August 2, 1846. The city has been home to many important events including Gandhi’s assassination (January 7th), Jawaharlal Nehru University riots(June-August)and more recently protests against corruption by Anna Hazare . In addition there are also several museums such as National Museum which houses artifacts dating back thousands years ago! There were even some rumors about an underground tunnel system connecting all these places together but no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Model: Question 2"
      ],
      "metadata": {
        "id": "UAP4Z_oX-8ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is Generative AI, Can you please help me understand it's usecases?\"\n",
        "\n",
        "device = 'cuda'\n",
        "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, temperature=0.1, top_k=1, top_p=1.0, repetition_penalty=1.4, min_new_tokens=16, max_new_tokens=128, do_sample=True)\n",
        "decoded = tokenizer.decode(generated_ids[0])\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f309b82e-6e37-4161-ac4a-ae960d0bd578",
        "id": "gb8_LXs5-srr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> What is Generative AI, Can you please help me understand it's usecases?\n",
            "- 01 May 2023\n",
            "- 4 Minutes to read\n",
            "\n",
            "# Overview of ChatGPT and GTP-3.5 models for generative artificial intelligence (AI) applications in the healthcare industry: Use cases & benefits\n",
            "\n",
            "Generating text using a large language model like OpenAI’s chatbot or other similar tools can be useful when writing content such as blog posts, articles, emails etc., but there are also many potential uses within medical settings where these types may come into play too! For example - if someone has trouble remembering what they said during an appointment with their doctor then\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = decoded.split(text)[-1]\n",
        "answer = answer.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8K7oO18-2no",
        "outputId": "cc5ab9d2-a008-485a-fcb9-e4b4378d26c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- 01 May 2023\n",
            "- 4 Minutes to read\n",
            "\n",
            "# Overview of ChatGPT and GTP-3.5 models for generative artificial intelligence (AI) applications in the healthcare industry: Use cases & benefits\n",
            "\n",
            "Generating text using a large language model like OpenAI’s chatbot or other similar tools can be useful when writing content such as blog posts, articles, emails etc., but there are also many potential uses within medical settings where these types may come into play too! For example - if someone has trouble remembering what they said during an appointment with their doctor then\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGE9vUrO_Pwn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}